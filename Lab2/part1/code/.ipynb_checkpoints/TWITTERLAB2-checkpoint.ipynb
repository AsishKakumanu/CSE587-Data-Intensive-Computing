{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name : Swapnika P <br/>\n",
    "UB Person No. : 50289464 <br/>\n",
    "UBIT Name : swapnika <br/>\n",
    "\n",
    "###### Teammate\n",
    "\n",
    "Name : Asish Kakumanu <br/>\n",
    "UB Person No. : 50288695 <br/>\n",
    "UBIT Name : asishkak <br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name : Asish Kakumanu <br/>\n",
    "UB Person No. : 50288695 <br/>\n",
    "UBIT Name : asishkak <br/>\n",
    "\n",
    "###### Teammate\n",
    "\n",
    "Name : Swapnika P <br/>\n",
    "UB Person No. : 50289464 <br/>\n",
    "UBIT Name : swapnika <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T21:55:46.733710Z",
     "start_time": "2019-04-01T21:55:45.691271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1112835863413579776#@SenGillibrand No, he isn't.  #Democrats like you are with your constant whining and lying. #RussiaRussiaRussia BS.â€¦ https://t.co/nUbAvyd8mv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import tweepy\n",
    "\n",
    "# Keys\n",
    "consumer_key = 'XKCvkSC3TQDBJlLfaMJGbyuyT'\n",
    "consumer_secret = 'SPkJ1siXeBh13GmwcrCYispF8eriDx7LQoIdMYfADodsG9neUe'\n",
    "access_token = '4808854766-2b02ddivk2dSwqgJgtg5JLDCN741tCN0Q26ax3a'\n",
    "access_secret = 'laVPlvPitjdmlhXi0KorC9ODYa83XhAq98LyUAEC6EHkr'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
    "auth.set_access_token(access_token,access_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "'''\n",
    "tweets = api.search(q=\"trump\",lang=\"en\",rpp=100,count=1000)\n",
    "'''\n",
    "\n",
    "query = 'trump -filter:retweets'\n",
    "max_tweets = 1\n",
    "date=\"2019-01-01\"\n",
    "searched_tweets = [status for status in tweepy.Cursor(api.search, q=query, since=date).items(max_tweets)]\n",
    "count = 1\n",
    "for tweet in searched_tweets:\n",
    "    if not tweet.retweeted and ('RT @' not in tweet.text):\n",
    "        print(str(tweet.id)+ \"#\" +tweet.text + \"\\n\")\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-03T09:02:45.317Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tweepy\n",
    "import time\n",
    "import datetime as dt\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Keys\n",
    "consumer_key = 'XKCvkSC3TQDBJlLfaMJGbyuyT'\n",
    "consumer_secret = 'SPkJ1siXeBh13GmwcrCYispF8eriDx7LQoIdMYfADodsG9neUe'\n",
    "access_token = '4808854766-2b02ddivk2dSwqgJgtg5JLDCN741tCN0Q26ax3a'\n",
    "access_secret = 'laVPlvPitjdmlhXi0KorC9ODYa83XhAq98LyUAEC6EHkr'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
    "auth.set_access_token(access_token,access_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "'''\n",
    "tweets = api.search(q=\"trump\",lang=\"en\",rpp=100,count=1000)\n",
    "'''\n",
    "\n",
    "query = 'trump border wall -filter:retweets'\n",
    "max_tweets = 200\n",
    "date = \"2019-01-01\"\n",
    "USA = '39.8,-95.583068847656,2500km'\n",
    "    \n",
    "\n",
    "count = 1\n",
    "while True:\n",
    "    try:\n",
    "        searched_tweets = [status for status in tweepy.Cursor(api.search, q=query, since=date, geoloc=USA).items(max_tweets)]\n",
    "        file = open(\"tweets.txt\",\"a\")\n",
    "        for tweet in searched_tweets:\n",
    "            if not tweet.retweeted and ('RT @' not in tweet.text):\n",
    "                print(tweet.text + \"\\n\")\n",
    "                file.write(tweet.text+\"\\n\")\n",
    "                file.write(\"\\n\")\n",
    "                count += 1\n",
    "        file.close()\n",
    "        #time.sleep(15*60)\n",
    "    except tweepy.RateLimitError:\n",
    "        print(\"Rate Limit Exceeded\")\n",
    "        file.close()\n",
    "        time.sleep(15*60)\n",
    "    except tweepy.TweepError as e:\n",
    "        #print(\"TweepError Raised {}\",e.message[0]['code'])\n",
    "        file.close()\n",
    "        time.sleep(15*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T21:26:40.015898Z",
     "start_time": "2019-04-08T21:26:40.013969Z"
    }
   },
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T22:34:04.130720Z",
     "start_time": "2019-04-19T22:34:01.796210Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from  nltk.corpus import stopwords\n",
    "import sys\n",
    "\n",
    "# Lowercase \n",
    "file = open('tweets.txt')\n",
    "file_lines = file.read()\n",
    "file_lines_lower = file_lines.lower()\n",
    "file_lines_lower\n",
    "\n",
    "# Remove username\n",
    "file_lines_remove_username = re.sub('@[^\\s]+','',file_lines_lower)\n",
    "\n",
    "# Remove links\n",
    "file_lines_remove_links = re.sub(r'http\\S+', '', file_lines_remove_username)\n",
    "\n",
    "# Remove Punctuation\n",
    "punctuations = '''!()-[]{};:'`\"\\,<>|=./?@#$%^&*_~'''\n",
    "lines_punctuated = \"\"\n",
    "\n",
    "for char in file_lines_remove_links:\n",
    "    if char not in punctuations:\n",
    "        lines_punctuated = lines_punctuated + char\n",
    "\n",
    "\n",
    "# Removing spaces \n",
    "l = lines_punctuated.lstrip()\n",
    "r = l.rstrip()\n",
    "total = r.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T22:46:39.404208Z",
     "start_time": "2019-04-19T22:46:39.057470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Asish/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Stopwords \n",
    "nltk.download('stopwords')\n",
    "cachedEnStopWords = stopwords.words(\"english\")\n",
    "cachedFrStopWords = stopwords.words(\"french\")\n",
    "cachedEsStopWords = stopwords.words(\"spanish\")\n",
    "\n",
    "textEn = ' '.join([word for word in total.splitlines() if word not in cachedEnStopWords])\n",
    "textFr = ' '.join([word for word in textEn.splitlines() if word not in cachedFrStopWords])\n",
    "textEs = ' '.join([word for word in textFr.splitlines() if word not in cachedEsStopWords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T00:19:49.429722Z",
     "start_time": "2019-04-20T00:19:49.265394Z"
    }
   },
   "outputs": [],
   "source": [
    "writeFile = open(\"total.txt\",\"a\")\n",
    "writeFile.write(total)\n",
    "writeFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T20:22:53.923932Z",
     "start_time": "2019-04-18T20:22:38.428683Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stemming \n",
    "nltk.download('punkt')\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stemmer= SnowballStemmer(\"english\",)\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(str(sentence))\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(stemmer.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "x=stemSentence(textEs)\n",
    "\n",
    "# Emoji\n",
    "def remove_emoji(string):\n",
    "    \n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                u\"\\U00002702-\\U000027B0\"\n",
    "                u\"\\U000024C2-\\U0001F251\"\n",
    "                u\"\\U0001f926-\\U0001f937\"\n",
    "                u'\\U00010000-\\U0010ffff'\n",
    "                u\"\\u200d\"\n",
    "                u\"\\u2640-\\u2642\"\n",
    "                u\"\\u2600-\\u2B55\"\n",
    "                u\"\\u23cf\"\n",
    "                u\"\\u23e9\"\n",
    "                u\"\\u231a\"\n",
    "                u\"\\u3030\"\n",
    "                u\"\\ufe0f\"\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "finaltext = remove_emoji(str(x))\n",
    "\n",
    "# Remove Numbers, Special Chars\n",
    "\n",
    "\n",
    "# Writing \n",
    "newFile = open(\"FinalTweets.txt\",\"a\",encoding='utf-8')\n",
    "t = re.split(r\"\\s{2,}\", finaltext)\n",
    "for sentence in t:\n",
    "    newFile.write(sentence + \"\\n\")\n",
    "\n",
    "    \n",
    "newFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T06:51:34.660350Z",
     "start_time": "2019-04-20T06:51:30.023512Z"
    }
   },
   "outputs": [],
   "source": [
    "# Final Stopword remover\n",
    "with open('swords.txt') as stopfile:\n",
    "    stopword = stopfile.read()\n",
    "    listSw = stopword.split()\n",
    "\n",
    "    \n",
    "with open(\"5output.txt\",\"r\") as readFile:\n",
    "    filedata = readFile.read()\n",
    "    \n",
    "total_sentences = filedata.splitlines()\n",
    "    \n",
    "text = ''\n",
    "for i in range(len(total_sentences)):\n",
    "    for word in total_sentences[i].split():\n",
    "        if word not in listSw:\n",
    "            text = text + word + ' '\n",
    "    #print(text)\n",
    "    text = text + '\\n'\n",
    "    \n",
    "import re\n",
    "myStr = re.sub('[^ a-zA-Z.|\\n]', '', text)\n",
    "writeFile = open(\"55out.txt\",\"a\")\n",
    "writeFile.write(text)\n",
    "writeFile.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "nteract": {
   "version": "0.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
